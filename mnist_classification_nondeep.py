# -*- coding: utf-8 -*-
"""MNIST_Classification_Nondeep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oTCrrUD0xnrGrq69CtjGazNdm4NTpMqy
"""

# !pip install idx2numpy

#train path
train_img_path = '/content/drive/MyDrive/mnist_project/train-images-idx3-ubyte'
train_label_path = '/content/drive/MyDrive/mnist_project/train-labels-idx1-ubyte'

#test path
test_img_path = '/content/drive/MyDrive/mnist_project/t10k-images-idx3-ubyte'
test_label_path = '/content/drive/MyDrive/mnist_project/t10k-labels-idx1-ubyte'

import numpy as np
import gzip
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import idx2numpy
import pandas as pd
import seaborn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import PCA

def create_data(train_img_path, train_label_path,test_img_path, test_label_path):

  #train data
  train_img = idx2numpy.convert_from_file(train_img_path)
  train_label = idx2numpy.convert_from_file(train_label_path)

  #test data
  test_img = idx2numpy.convert_from_file(test_img_path)
  test_label = idx2numpy.convert_from_file(test_label_path)
    
  #Reshaping the dataset
  train_x = train_img.reshape(train_img.shape[0],-1)
  test_x = test_img.reshape(test_img.shape[0],-1)
  print (train_x.shape, test_x.shape)

  #return
  return train_x, train_label, test_x, test_label

train_img, train_label, test_img, test_label = create_data(train_img_path, train_label_path,test_img_path,
                                                       test_label_path)

"""#### Implement PCA"""

#Implement PCA and LDA to the training and test set

def get_pca_data(train_img,test_img, pca_components) : 

  #Reshape the data
  train_x = train_img.reshape(train_img.shape[0],-1)
  test_x = test_img.reshape(test_img.shape[0],-1) 

  #1. Standardize the data
  scaler = StandardScaler()
  scaler.fit(train_x)

  #Transform the data
  train_x_trans = scaler.transform(train_x)
  test_x_trans = scaler.transform(test_x)

  #2. Implement PCA to the train data
  #pca_components = 300
  pca = PCA(n_components=pca_components)
  pca.fit(train_x_trans)

  #Apply PCA to the train and test datas
  train_x_pca = pca.transform(train_x_trans)
  test_x_pca = pca.transform(test_x_trans)

  #Check the variance of the fit

  var_exp = pca.explained_variance_ratio_.sum()
  print ('Variance Explained ' + str(var_exp * 100) + ' ' + str(pca_components)+ ' components')

  #return the reduced data
  return train_x_pca, test_x_pca

def get_lda_data(train_img,train_label, test_img, lda_components) : 

  #Reshape the data
  train_x = train_img.reshape(train_img.shape[0],-1)
  test_x = test_img.reshape(test_img.shape[0],-1) 


  #2. Implement LDA to the train data
  #pca_components = 300
  lda = LDA(n_components=lda_components)
  lda.fit(train_x,train_label)

  #Apply PCA to the train and test datas
  train_x_lda = lda.transform(train_x)
  test_x_lda = lda.transform(test_x)

  #return the reduced data
  return train_x_lda, test_x_lda

train_x_pca, test_x_pca= get_pca_data(train_img,test_img, pca_components=2)

#Create data to visualize
df_visual = pd.DataFrame(train_x_pca)
df_visual.columns = ['Component1','Component2']
df_visual['labels'] = train_label
sns.scatterplot(data=df_visual, x="Component1", y="Component2", hue="labels")
plt.title('PCA Visualization')

train_x_lda, test_x_lda= get_lda_data(train_img,train_label, test_img, lda_components = 2)

#Create data to visualize
df_visual = pd.DataFrame(train_x_lda)
df_visual.columns = ['Component1','Component2']
df_visual['labels'] = train_label
sns.scatterplot(data=df_visual, x="Component1", y="Component2", hue="labels")
plt.title('MDA Visualization')



"""### Fit Logistic Regression Model with PCA"""

# #Define get the pca data
# total_var = []
# for components in ([10,20,50,100,200,250,300]) : 
#     _, _, var_exp = get_pca_data(train_img,test_img, pca_components = components)
#     total_var.append(var_exp)

# plt.plot([9,20,50,100,200,250,300], total_var)
# plt.xlabel('Principal Components')
# plt.ylabel('Variance Explained')

#Fit logistic regression
def fit_logistic_pca (train_img, train_label, test_img, test_label, pca_components):

  #get the train and test features
  train_x_pca, test_x_pca = get_pca_data(train_img,test_img, pca_components = pca_components)

  #train test split
  X_train, X_val, y_train, y_val = train_test_split(train_x_pca, train_label, test_size=0.2, random_state=42)

  #fir the model
  lgr = LogisticRegression(random_state=0).fit(X_train, y_train)
  pred_train = lgr.predict(X_train)
  pred_val =   lgr.predict(X_val)
  pred_test =  lgr.predict(test_x_pca)
  
  #get the acc score
  score_train = lgr.score(X_train, y_train)
  score_val =  lgr.score(X_val, y_val)
  score_test = lgr.score(test_x_pca, test_label)

  #print
  print ('The train accuracy is ' + str(score_train))
  print ('The validation accuracy is ' + str(score_val))
  print ('The test accuracy is ' + str(score_test))


  #return the scores
  return score_train, score_val, score_test

score_train, score_val, score_test = fit_logistic_pca(train_img, train_label, test_img, test_label, 250)



"""### Fit Logistic Regression Model with LDA"""

#Fit logistic regression
def fit_logistic_lda(train_img,test_img, train_label, test_label, lda_components):

  #get the train and test features
  train_x_lda, test_x_lda = get_lda_data(train_img,train_label,test_img, lda_components = 9)

  #train test split
  X_train, X_val, y_train, y_val = train_test_split(train_x_lda, train_label, test_size=0.2, random_state=42)

  #fir the model
  lgr = LogisticRegression(random_state=0).fit(X_train, y_train)
  pred_train = lgr.predict(X_train)
  pred_val =   lgr.predict(X_val)
  pred_test =  lgr.predict(test_x_lda)
  
  #get the acc score
  score_train = lgr.score(X_train, y_train)
  score_val =  lgr.score(X_val, y_val)
  score_test = lgr.score(test_x_lda, test_label)

  #print
  print ('The train accuracy is ' + str(score_train))
  print ('The validation accuracy is ' + str(score_val))
  print ('The test accuracy is ' + str(score_test))


  #return the scores
  return score_train, score_val, score_test

score_train, score_val, score_test = fit_logistic_lda(train_img,test_img, train_label, 
                                                   test_label, 9)



"""### Fit Kernel, Polynomial and Linear SVM with PCA"""

# C=1.0, kernel='rbf', degree=3, gamma='scale',

#Fit logistic regression
def fit_svm_pca (train_img,test_img, train_label, test_label, pca_components, 
                    C, kernel, degree, gamma):


  #get the train and test features
  train_x_pca, test_x_pca = get_pca_data(train_img,test_img, pca_components = pca_components)

  #train test split
  X_train, X_val, y_train, y_val = train_test_split(train_x_pca, train_label, test_size=0.2, random_state=42)

  #fir the model
  svm_model = SVC(C = C, gamma = 'auto', degree =degree, kernel = kernel).fit(X_train, y_train)

  #Hyperparameters
  #1. C : regularization, default = 1
  #2. kernel = 'linear', 'poly', 'rbf'
  #3. degree = 3,4.. for 'poly'
  #4. Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. 
  #if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,
  #if ‘auto’, uses 1 / n_features.

  #predict
  pred_train = svm_model.predict(X_train)
  pred_val =   svm_model.predict(X_val)
  pred_test =  svm_model.predict(test_x_pca)
  
  #get the acc score
  score_train = svm_model.score(X_train, y_train)
  score_val =  svm_model.score(X_val, y_val)
  score_test = svm_model.score(test_x_pca, test_label)

  #print
  print ('The train accuracy is ' + str(score_train) + ' with ' + str(kernel) + ' kernel')
  print ('The validation accuracy is ' + str(score_val) + ' with ' + str(kernel) + ' kernel')
  print ('The test accuracy is ' + str(score_test) + ' with ' + str(kernel) + ' kernel')


  #return the scores
  return score_train, score_val, score_test

pca_components = 50
C = 1
kernel = 'linear'
degree = 3
gamma = 'auto'

score_train, score_val, score_test = fit_svm_pca (train_img,test_img, train_label, 
                                              test_label, pca_components, 
                                              C, kernel, degree, gamma)



"""### Fit Kernel, Polynomial and Linear SVM with LDA"""

#Fit logistic regression
def fit_svm_lda (train_img,test_img, train_label, test_label, lda_components, 
                    C, kernel, degree, gamma):


  #get the train and test features
  train_x_lda, test_x_lda = get_lda_data(train_img,train_label,test_img, lda_components = 9)

  #train test split
  X_train, X_val, y_train, y_val = train_test_split(train_x_lda, train_label, test_size=0.2, random_state=42)

  #fir the model
  svm_model = SVC(C = C, gamma = 'auto', degree =degree, kernel = kernel).fit(X_train, y_train)

  #Hyperparameters
  #1. C : regularization, default = 1
  #2. kernel = 'linear', 'poly', 'rbf'
  #3. degree = 3,4.. for 'poly'
  #4. Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. 
  #if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,
  #if ‘auto’, uses 1 / n_features.

  #predict
  pred_train = svm_model.predict(X_train)
  pred_val =   svm_model.predict(X_val)
  pred_test =  svm_model.predict(test_x_lda)
  
  #get the acc score
  score_train = svm_model.score(X_train, y_train)
  score_val =  svm_model.score(X_val, y_val)
  score_test = svm_model.score(test_x_lda, test_label)

  #print
  print ('The train accuracy is ' + str(score_train) + ' with ' + str(kernel) + ' kernel')
  print ('The validation accuracy is ' + str(score_val) + ' with ' + str(kernel) + ' kernel')
  print ('The test accuracy is ' + str(score_test) + ' with ' + str(kernel) + ' kernel')


  #return the scores
  return score_train, score_val, score_test

lda_components = 9
C = 1
kernel = 'rbf'
degree = 3
gamma = 'auto'

score_train, score_val, score_test = fit_svm_lda (train_img,test_img, train_label, 
                                              test_label, lda_components, 
                                              C, kernel, degree, gamma)













