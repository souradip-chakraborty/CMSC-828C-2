# -*- coding: utf-8 -*-
"""MNIST_Classification_Deep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jXEGsJDn17ft9n24iRtVmgaqwUf3YNOJ
"""

# !pip install idx2numpy

#train path
train_img_path = '/content/drive/MyDrive/mnist_project/train-images-idx3-ubyte'
train_label_path = '/content/drive/MyDrive/mnist_project/train-labels-idx1-ubyte'

# #test path
test_img_path = '/content/drive/MyDrive/mnist_project/t10k-images-idx3-ubyte'
test_label_path = '/content/drive/MyDrive/mnist_project/t10k-labels-idx1-ubyte'

#!pip install torchsummary 
from torchsummary import summary

import numpy as np
import gzip
import matplotlib.pyplot as plt
import idx2numpy
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
# from sklearn.decomposition import PCA
# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from torch.utils.data import TensorDataset, DataLoader

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

def create_data(train_img_path, train_label_path,test_img_path, test_label_path):

  #train data
  train_img = idx2numpy.convert_from_file(train_img_path)
  train_label = idx2numpy.convert_from_file(train_label_path)

  #test data
  test_img = idx2numpy.convert_from_file(test_img_path)
  test_label = idx2numpy.convert_from_file(test_label_path)
    
  #Reshaping the dataset
  #train_x = train_img.reshape(train_img.shape[0],-1)
  #test_x = test_img.reshape(test_img.shape[0],-1)
  print (train_img.shape, test_img.shape)

  #return
  return train_img, train_label, test_img, test_label

train_x, train_label, test_x, test_label = create_data(train_img_path, train_label_path,test_img_path,
                                                       test_label_path)

"""#### Implement a Deep Convolutional Neural Network Model"""

# #Hyperparameters
n_epochs = 5
batch_size_train = 64
batch_size_test = 1000
learning_rate = 0.01
momentum = 0.5
log_interval = 10

random_seed = 1
torch.backends.cudnn.enabled = False
torch.manual_seed(random_seed)

def create_torch_dataset(train_x, train_label, test_x, test_label,BATCH_SIZE) :
  
  #Create a Pytorch Dataset
  tensor_train_imgs = torch.Tensor(np.expand_dims(train_x, axis=1)) # transform to torch tensor
  tensor_train_labels = torch.Tensor(train_label)

  tensor_test_imgs = torch.Tensor(np.expand_dims(test_x, axis=1)).float() # transform to torch tensor
  tensor_test_labels = torch.Tensor(test_label)

  train_dataset = TensorDataset(tensor_train_imgs,tensor_train_labels) # create your datset
  test_dataset =  TensorDataset(tensor_test_imgs,tensor_test_labels) # create your datset

  # data loader
  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = False)
  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)

  return train_loader, test_loader

train_loader, test_loader = create_torch_dataset(train_x, train_label, test_x, test_label,BATCH_SIZE = 64)

examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Ground Truth: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig

"""#### Building the CNN Network"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

network = Net()
optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)
criterion = nn.CrossEntropyLoss()

network

len(train_loader)

def train(epoch):

    #define the list
    running_loss = 0.0

    #set the parameters to train
    network.train()

    #looping through the batch
    for batch_idx, (data, target) in enumerate(train_loader):
        
        #removing accumulated gradient
        optimizer.zero_grad()
        #output of the model
        output = network(data)
        #negative log likelihood
        loss = F.nll_loss(output, target.long())
        loss.backward()
        #gradient descent
        optimizer.step()


        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100. * batch_idx / len(train_loader), loss.item()))
        
        #batch
        #train_losses.append(loss.item())

        #get the loss
        running_loss += loss.item()
            
        
    return running_loss/ len(train_loader.dataset)

# train_losses

def test():

    #define the test loss
    #test_losses = []

    network.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            
            output = network(data)
            test_loss += F.nll_loss(output, target.long(), size_average=False).item()
            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()
        
        test_loss /= len(test_loader.dataset)
        #test_losses.append(test_loss)
        print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

    return test_loss

test()

n_epochs = 25

total_tr_loss = []
total_te_loss = []
for epoch in range(1, n_epochs + 1):
    loss_tr = train(epoch)
    loss_te = test()
    total_tr_loss.append(loss_tr)
    total_te_loss.append(loss_te)

fig = plt.figure()
plt.plot(total_tr_loss, color='blue')
# plt.plot(total_te_loss, color='red')
#plt.legend(['train','test'])
plt.xlabel('No of iterations')
plt.ylabel('negative train log likelihood loss')

fig = plt.figure()
plt.plot(total_te_loss, color='red')
#plt.legend(['train','test'])
plt.xlabel('No of iterations')
plt.ylabel('negative test log likelihood loss')

plt.plot(train_losses)
plt.ylabel('Train loss')
plt.xlabel('Mini-Batch Epochs')
# plt.xlim(-1,100)

with torch.no_grad():
  output = network(example_data)

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Prediction: {}".format(
    output.data.max(1, keepdim=True)[1][i].item()))
  plt.xticks([])
  plt.yticks([])
fig





